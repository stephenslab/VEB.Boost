% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/veb_boost.R
\name{veb_boost}
\alias{veb_boost}
\title{Performs VEB-Boosting}
\usage{
veb_boost(
  X,
  Y,
  fitFunctions,
  predFunctions,
  constCheckFunctions,
  growTree = TRUE,
  k = 1,
  d = 1,
  growMode = c("+", "*", "+*"),
  changeToConstant = TRUE,
  family = c("gaussian", "binomial", "multinomial"),
  tol = length(Y)/10000,
  verbose = TRUE,
  mc.cores = 1
)
}
\arguments{
\item{X}{is a list of predictor objects with either 1 or k elements. If it contains 1 element, then the same
predictor is used for each base learner. Otherwise, the k sub-trees that we add together each get their own X.
Each predictor can take any form, so long as the user-supplied fitFunctions and predFunctions know how to use them.}

\item{Y}{is a numeric vector response}

\item{fitFunctions}{is either a single fitting function, or a list of length `k` of fitting functions to be used in
each term on the sum of nodes}

\item{predFunctions}{is either a single prediction function, or a list of length `k` of prediction functions to be used in
each term of the sum of nodes}

\item{constCheckFunctions}{is either a single constant check function, or a list of length `k` of constant check functions
to be used in each term of the sum of nodes}

\item{growTree}{is a logical for if we should grow the tree after convergence (TRUE), or only use the initial tree
structure (FALSE)}

\item{k}{is an integer for how many terms are in the sum of nodes}

\item{d}{is either an integer, or an integer vector of length `k` for the multiplicative depth of each of the k terms
NOTE: This can be dangerous. For example, if the fit starts out too large, then entire branhces will be fit to be exactly
zero. When this happens, we end up dividing by 0 in places, and this results in NAs, -Inf, etc. USE AT YOUR OWN RISK}

\item{growMode}{specifies how we grow the tree, either splitting nodes with addition, multiplication, or both
If `+`, we grow mu_0 -> (mu_0 + mu_1)
If `*`, we grow mu_0 -> (mu_0 * mu_1) (NOTE: Not recommended if we start with `k = 1`)
If `+*``, we grow mu_0 -> (mu_0 * mu_2) + mu_1}

\item{changeToConstant}{is a flag for if, when the fit is found to be basically constant, if we should actually change
the fitting function of that node to fit exactly a constant value}

\item{family}{is what family the response is}

\item{tol}{is a positive scalar specifying the level of convergence to be used}

\item{verbose}{is a logical flag specifying whether we should report convergence information as we go}

\item{mc.cores}{is the number of cores to use in mclapply, only used in family == "multinomial", and only
supported on UNIX systems, where mclapply works. NOT CURRENTLY SUPPORTED}
}
\value{
A \code{VEB_Boost_Node} object with the fit
}
\description{
Solves the VEB-Boost regression problem using the supplied inputs
}
\details{
Given a pre-specified arithmetic tree structure \deqn{T(\mu_1, \dots, \mu_L)},
priors \deqn{\mu_l \sim g_l(\cdot)}, and inputs for the response, VEB-Boosting is performed.

A cyclic CAVI scheme is used, where we cycle over the leaf nodes and update the approxiomation
to the posterior distribution at each node in turn.

We start with the arithmetic tree structure \deqn{T(\mu_1, \dots, \mu_L) = \sum_{i=1}^k \prod_{j=1}^{d_k} \mu_{i, j}}
}
\examples{

set.seed(1)
n = 1000
p = 1000
X = matrix(runif(n * p), nrow = n, ncol = p)
Y = rnorm(n, 5*sin(3*X[, 1]) + 2*(X[, 2]^2) + 3*X[, 3]*X[, 4])

For input X and list `fit` returned from fitFn (encoding the variational posterior for b), computes either the 1st or 2nd 
predFn = function(X, fit, moment = c(1, 2)) {
  # posterior moments of the response (depending on if `moment` is 1 or 2)
  if (moment == 1) {
    res = E_fit[Xb]
  } else {
    res = E_fit[(Xb)^2]
  }
  return(res)
}

For a given prior g(b), a function that approximates the posterior of b, q(b), using Variational Inference
fitFn = function(X, Y, sigma2, init) {
  fit = list(whatever is needed to encode the variational posterior)
  KL_div = D_KL(q || g) # KL divergence from variational posterior to prior
  mu1 = predFn(X, fit, 1)
  mu2 = predFn(X, fit, 2)
  # add mu1, mu2, and KL_div to the fit, MUST BE CALLED $mu1, $mu1, and $KL_div
  fit$mu1 = mu1
  fit$mu2 = mu2
  fit$KL_div = KL_div
  return(fit)
}

For a given `fit`, returns TRUE if the variational posterior is close enough to a constant, else FALSE
constCheckFn = function(fit) {
  if (fit is close to constant) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}
veb.fit = veb_boost(list(X), Y, fitFn, predFn, constCheckFn, family = "gaussian")

}
